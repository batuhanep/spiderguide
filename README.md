# Web Scraping & Spider Guide (Python + Scrapy)

> **AmaÃ§:** SÄ±fÄ±rdan baÅŸlayÄ±p, HTMLâ€™i okuyabilen, XPath/CSS seÃ§icilerle veri Ã§Ä±karan, linkleri takip edip sayfalar arasÄ± gezebilen (crawl) bir **Scrapy** Ã¶rÃ¼mceÄŸi (spider) kurmak. Bu rehberi doÄŸrudan GitHub repoya **`WEB_SCRAPING_SPIDER_GUIDE.md`** olarak koyabilirsin.

---

## Ä°Ã§indekiler
1. Ã–n KoÅŸullar ve Kurulum
2. HTMLâ€™e HÄ±zlÄ± GiriÅŸ
3. XPath SeÃ§iciler: Temelden Ä°leriye
4. CSS SeÃ§iciler ve XPath â†” CSS DÃ¶nÃ¼ÅŸÃ¼mleri
5. Attribute ve Metin (Text) Ã‡Ä±karma
6. Scrapy Selector ile Ã‡alÄ±ÅŸma (Standalone)
7. Response Nesnesiyle Ã‡alÄ±ÅŸma ve Link Takibi
8. Ä°lk Ã–rÃ¼mceÄŸini Yaz: CrawlerProcess ile Ã‡alÄ±ÅŸtÄ±rma
9. Crawl: Linkleri Ã‡ek, SayfalarÄ± Gez, Veri Topla
10. Mini Capstone: Kurs â†’ BÃ¶lÃ¼m BaÅŸlÄ±klarÄ± SÃ¶zlÃ¼ÄŸÃ¼
11. Kontrol Listesi (Debug / Ä°pucu)
12. SÄ±k KarÅŸÄ±laÅŸÄ±lan Hatalar ve Ã‡Ã¶zÃ¼mler
13. Ek Ã–devler ve GeliÅŸtirmeler

---

## Ã–n KoÅŸullar ve Kurulum

### 1) Python ve Sanal Ortam
```bash
python -V          # 3.10+ Ã¶nerilir
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 2) Scrapy Kurulumu
```bash
pip install scrapy requests parsel
```
> `parsel`, Scrapyâ€™nin altÄ±nda da kullanÄ±lan selektÃ¶r kÃ¼tÃ¼phanesidir; HTML Ã¼zerinde XPath ve CSS kullanacaÄŸÄ±z.

### 3) PyCharm Ä°pucu
- Yeni bir **Virtualenv** proje oluÅŸtur; **Interpreter** olarak `.venv`â€™i seÃ§.
- Run/Debug konfigÃ¼rasyonunda Ã§alÄ±ÅŸtÄ±racaÄŸÄ±n dosyayÄ± seÃ§ (Ã¶r. `spider_dc.py`).

---

## HTMLâ€™e HÄ±zlÄ± GiriÅŸ
HTML aÄŸaÃ§ yapÄ±sÄ±nda etiketler (tags) iÃ§erir. En temel etiketlerden bazÄ±larÄ±: `html`, `body`, `div`, `p`, `a`. Elementlerin **attribute**â€™larÄ± vardÄ±r: `id` (benzersiz olmalÄ±), `class` (birden fazla elemana verilebilir), `href` (link hedefi), `src` (gÃ¶rsel kaynaÄŸÄ±) vb.

**Ã–ÄŸrendiklerimiz:**
- AÄŸaÃ§ mantÄ±ÄŸÄ±yla ebeveyn/Ã§ocuk (parent/child) ve kardeÅŸ (sibling) kavramlarÄ±.
- `<a>` baÄŸlantÄ± etiketi ve `href`.
- `id` benzersiz, `class` tekrar edebilir.

> Bu bÃ¶lÃ¼mdeki kavramlar ileride XPath/CSS ile hedef seÃ§iminde kritik olacak.

---

## XPath SeÃ§iciler: Temelden Ä°leriye

### Temel Semboller
- `/` : Bir alt nesile git (tek adÄ±m)
- `//` : AÅŸaÄŸÄ± doÄŸru tÃ¼m nesillerde ara
- `[]` : KardeÅŸler arasÄ±ndan indeksleme veya filtreleme (`p[2]` gibi)
- `*` : Joker (herhangi bir tag)
- `@attr` : Attribute eriÅŸimi (`@href`, `@class`, `@id`)
- `contains(@attr, "parÃ§a")` : Attribute iÃ§inde parÃ§a eÅŸleÅŸtirme

### Ã–rnekler
```xpath
/html/body/p             # body altÄ±ndaki tÃ¼m p'ler
/html/body/div/p[2]      # body>div altÄ±ndaki ikinci p
//p                      # sayfadaki tÃ¼m p'ler
/html/body/*             # body altÄ±ndaki tÃ¼m Ã§ocuklar
//*[@id="uid"]          # id'si uid olan herhangi bir element
//div[@id="uid"]/p[2]   # id=uid div'i altÄ±ndaki ikinci p
//*[contains(@class,"class-1")] # class iÃ§inde class-1 geÃ§enler
//div[@id="uid"]/a/@href       # href attribute deÄŸerleri
```

**Not:** `'/html/body'` ile `'/html[1]/body[1]'` aynÄ± hedefi verir. XPathâ€™ta indeksler 1â€™den baÅŸlar.

---

## CSS SeÃ§iciler ve XPath â†” CSS DÃ¶nÃ¼ÅŸÃ¼mleri

### HÄ±zlÄ± EÅŸleÅŸtirmeler
- `XPath /`  â†’ `CSS >` (ilk karakter hariÃ§)
- `XPath //` â†’ `CSS`â€™te araya boÅŸluk (descendant) veya `>` (child) kullan
- `XPath [N]` â†’ `CSS :nth-of-type(N)`

**Ã–rnek dÃ¶nÃ¼ÅŸÃ¼mler**
```
XPath: /html/body//div/p[2]
CSS  : html > body div > p:nth-of-type(2)
```

### SÄ±k KullanÄ±lan CSS KalÄ±plarÄ±
```css
p.class-1                /* class'Ä± class-1 olan p'ler */
div#uid                  /* id'si uid olan div */
.class1                  /* class1 sÄ±nÄ±fÄ±na sahip tÃ¼m elemanlar */
div#uid > p.class1       /* id=uid div'inin altÄ±ndaki p.class1 */
```

### CSS ile Attribute/Metin
```css
a::attr(href)            /* a etiketlerinin href deÄŸeri */
p#p-example::text        /* p'nin sadece doÄŸrudan metin Ã§ocuklarÄ± */
p#p-example ::text       /* p ve altÄ±ndaki tÃ¼m metin dÃ¼ÄŸÃ¼mleri */
```

---

## Attribute ve Metin (Text) Ã‡Ä±karma

### XPath ile Attribute
```xpath
//div[@id='uid']/a/@href
```

### CSS ile Attribute
```css
div#uid > a::attr(href)
```

### Metin Ã‡Ä±karma
- XPath `text()` yalnÄ±zca **doÄŸrudan** metinleri dÃ¶ndÃ¼rÃ¼r.
- `//text()` torun metinleri de kapsar.
- CSSâ€™te `::text` doÄŸrudan metin; ` ::text` (boÅŸlukla) torunlarÄ± da iÃ§erir.

```python
sel.xpath("//p[@id='p-example']/text()").extract()
sel.xpath("//p[@id='p-example']//text()").extract()
sel.css("p#p-example::text").extract()
sel.css("p#p-example ::text").extract()
```

---

## Scrapy Selector ile Ã‡alÄ±ÅŸma (Standalone)
Bazen tam proje kurmadan da seÃ§ici pratikleri yapmak isteyebilirsin.

```python
from scrapy import Selector
html = """
<html>
  <body>
    <div class="hello datacamp">
      <p>Hello World!</p>
    </div>
    <p>Enjoy DataCamp!</p>
  </body>
</html>
"""
sel = Selector(text=html)

# XPath
sel.xpath("//p")                # SelectorList dÃ¶ner
sel.xpath("//p").extract()      # ['<p>Hello World!</p>', '<p>Enjoy DataCamp!</p>']
sel.xpath("//p").extract_first()# '<p>Hello World!</p>'

# CSS
sel.css("div > p").extract()    # ['<p>Hello World!</p>']

# requests ile gerÃ§ek sayfa -> Selector
import requests
url = "https://en.wikipedia.org/wiki/Web_scraping"
html = requests.get(url).content
sel = Selector(text=html)
```

> **Ä°pucu:** `SelectorList` indekslenebilir (`ps[1]` gibi). `extract()` HTML string dÃ¶ndÃ¼rÃ¼r; yalnÄ±zca metin istiyorsan Ã¶nce `::text`/`text()` ile nodeâ€™larÄ± hedefle.

---

## Response Nesnesiyle Ã‡alÄ±ÅŸma ve Link Takibi
Scrapy spider iÃ§inde `response` nesnesi **Selector** gibi davranÄ±r: `response.xpath(...)`, `response.css(...)`, ardÄ±ndan `extract()` / `extract_first()` ile veri alÄ±rsÄ±n.

Ek avantajlar:
- **`response.url`**: Åu anki sayfanÄ±n URLâ€™i.
- **`response.follow(next_url, callback=...)`**: Ä°lgili linki takip edip sonucu baÅŸka bir parse fonksiyonuna gÃ¶nderir.

```python
# zincirleme kullanÄ±m
data = response.xpath('//div').css('span.bio').extract()
# link takip
yield response.follow(url=next_url, callback=self.parse_item)
```

---

## Ä°lk Ã–rÃ¼mceÄŸini Yaz: `CrawlerProcess` ile Ã‡alÄ±ÅŸtÄ±rma
AÅŸaÄŸÄ±daki Ã¶rÃ¼mcek, bir sayfayÄ± isteyip HTMLâ€™ini dosyaya yazar. Tek dosyalÄ±k Ã§alÄ±ÅŸtÄ±rmalar iÃ§in idealdir.

```python
# file: spider_dc.py
import scrapy
from scrapy.crawler import CrawlerProcess

class DCSpider(scrapy.Spider):
    name = "dc_spider"

    def start_requests(self):
        url = "https://www.datacamp.com/courses/all"
        yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        with open("DC_courses.html", "wb") as fout:
            fout.write(response.body)

if __name__ == "__main__":
    process = CrawlerProcess()
    process.crawl(DCSpider)
    process.start()
```

> `start_requests` en az bir `scrapy.Request` Ã¼retmeli; `callback` hangi fonksiyonun `response`â€™u iÅŸleyeceÄŸini belirler.

---

## Crawl: Linkleri Ã‡ek, SayfalarÄ± Gez, Veri Topla
AynÄ± sayfadaki kurs kartlarÄ±ndan linkleri toplayÄ±p CSVâ€™ye yazalÄ±m; ardÄ±ndan her linki **follow** edip ikinci parse fonksiyonuna gÃ¶nderelim.

```python
# file: spider_links.py
import scrapy
from scrapy.crawler import CrawlerProcess

class DCLinksSpider(scrapy.Spider):
    name = "dc_links"

    def start_requests(self):
        yield scrapy.Request(
            url="https://www.datacamp.com/courses/all",
            callback=self.parse
        )

    def parse(self, response):
        # tek adÄ±mda
        links = response.css('div.course-block > a::attr(href)').extract()

        # veya adÄ±m adÄ±m
        # blocks = response.css('div.course-block')
        # hrefs = blocks.xpath('./a/@href')
        # links = hrefs.extract()

        # dosyaya yaz
        with open("DC_links.csv", "w", encoding="utf-8") as f:
            for link in links:
                f.write(link + "\n")

        # her linki takip et
        for link in links:
            yield response.follow(url=link, callback=self.parse_course)

    def parse_course(self, response):
        # burada her kurs sayfasÄ±nÄ± parse edebilirsin
        self.logger.info("Parsed course page: %s", response.url)

if __name__ == "__main__":
    process = CrawlerProcess()
    process.crawl(DCLinksSpider)
    process.start()
```

> **Not:** Sunumda `"/n"` kaÃ§Ä±ÅŸ dizisi yer alÄ±yordu; Pythonâ€™da satÄ±r sonu `"\n"` olmalÄ±dÄ±r.

---

## Mini Capstone: Kurs â†’ BÃ¶lÃ¼m BaÅŸlÄ±klarÄ± SÃ¶zlÃ¼ÄŸÃ¼
Bu Ã¶rnekte, Ã¶n sayfadan kurs linklerini toplayÄ±p her kurs sayfasÄ±nda **baÅŸlÄ±k** ve **bÃ¶lÃ¼m (chapter) baÅŸlÄ±klarÄ±nÄ±** alÄ±p bir sÃ¶zlÃ¼ÄŸe kaydediyoruz.

```python
# file: spider_capstone.py
import json
import scrapy
from scrapy.crawler import CrawlerProcess

dc_dict = {}

class DCChapterSpider(scrapy.Spider):
    name = "dc_chapter_spider"

    def start_requests(self):
        url = "https://www.datacamp.com/courses/all"
        yield scrapy.Request(url=url, callback=self.parse_front)

    def parse_front(self, response):
        course_blocks = response.css('div.course-block')
        course_links = course_blocks.xpath('./a/@href')
        for url in course_links.extract():
            yield response.follow(url=url, callback=self.parse_pages)

    def parse_pages(self, response):
        # Kurs baÅŸlÄ±ÄŸÄ±
        title_sel = response.xpath('//h1[contains(@class, "title")]/text()')
        course_title = (title_sel.extract_first() or '').strip()

        # Chapter baÅŸlÄ±klarÄ±
        ch_titles = response.css('h4.chapter__title::text')
        chapters = [t.strip() for t in ch_titles.extract()]

        if course_title:
            dc_dict[course_title] = chapters

    def closed(self, reason):
        with open('dc_chapters.json', 'w', encoding='utf-8') as f:
            json.dump(dc_dict, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    process = CrawlerProcess()
    process.crawl(DCChapterSpider)
    process.start()
```

> `closed(self, reason)` spider bittiÄŸinde Ã§aÄŸrÄ±lÄ±r; toplanan sÃ¶zlÃ¼ÄŸÃ¼ JSONâ€™a yazdÄ±rmak iÃ§in ideal.

---

## Kontrol Listesi (Debug / Ä°pucu)
- **Elementleri incele:** TarayÄ±cÄ±da â€œInspect Elementâ€ ile gerÃ§ek HTMLâ€™ye bak.
- **Selector test et:** `scrapy shell URL` ile interaktif denemeler yap (`response.css(...)`, `response.xpath(...)`).
- **Kademeli geliÅŸtir:** Ã–nce doÄŸru elementi seÃ§; sonra attribute/metin Ã§Ä±kar; sonra listeyi dÃ¶ngÃ¼ye sok; en son dosyaya yaz.
- **BaÄŸÄ±l (relative) vs mutlak XPath:** `./a/@href` mevcut nodeâ€™un Ã§ocuÄŸunu hedefler; en baÅŸa `//` koyarsan belki istenmeyen Ã§ok sonuÃ§ gelir.
- **Indexâ€™ler 1â€™den baÅŸlar:** XPath `p[1]`, `p[2]` â€¦
- **`::text` vs `//text()` farkÄ±:** DoÄŸrudan metin mi tÃ¼m alt metinler mi istediÄŸine gÃ¶re seÃ§.

---

## SÄ±k KarÅŸÄ±laÅŸÄ±lan Hatalar ve Ã‡Ã¶zÃ¼mler
- **BoÅŸ liste/None gelmesi:** SeÃ§ici yanlÄ±ÅŸ; sayfa dinamik olabilir. Ã–nce HTMLâ€™nin geldiÄŸini `response.text[:500]` ile teyit et; gerekirse `selenium` ya da API yoluna bak.
- **GÃ¶reli linkler:** `response.follow` Ã§oÄŸu zaman gÃ¶reli linkleri otomatik Ã§Ã¶zer. Kendi baÅŸÄ±na iÅŸliyorsan `urljoin` kullan.
- **Encode/escape sorunlarÄ±:** Dosyaya yazarken `encoding='utf-8'` kullan. SatÄ±r sonu iÃ§in `"\n"`.

---

## Ek Ã–devler ve GeliÅŸtirmeler
- Ã‡ektiÄŸin linklerden her kurs sayfasÄ±nda eÄŸitmen adlarÄ±nÄ±/etiketlerini de topla.
- Sitede sayfalama varsa `next` linkini `response.follow` ile takip ederek Ã§ok sayfalÄ± crawl yap.
- Ã‡Ä±kardÄ±ÄŸÄ±n veriyi **Pipelines** Ã¼zerinden CSV/JSON/SQLiteâ€™e aktaracak mini bir Scrapy projesi kur.

---

### Proje YapÄ± Ã–nerisi (tek dosyalÄ±k Ã¶rÃ¼mceklerle)
```
repo/
â”œâ”€ WEB_SCRAPING_SPIDER_GUIDE.md  # bu rehber
â”œâ”€ spider_dc.py                   # ilk HTML yazdÄ±rma Ã¶rneÄŸi
â”œâ”€ spider_links.py                # link Ã§ek + follow + CSV
â””â”€ spider_capstone.py             # kurs -> bÃ¶lÃ¼mler sÃ¶zlÃ¼ÄŸÃ¼ (JSON)
```

> Daha bÃ¼yÃ¼k projelerde `scrapy startproject projename` komutu ile klasÃ¶rlÃ¼ yapÄ± (spiders, items, pipelines, settings) tercih edilir.

---

**Mutlu kazÄ±malar!** ğŸ‘‹
